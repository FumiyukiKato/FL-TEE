{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# from opacus import PrivacyEngine\n",
    "# from opacus.utils.uniform_sampler import UniformWithReplacementSampler\n",
    "\n",
    "from scipy.special import loggamma\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "import datetime\n",
    "from logging import getLogger, StreamHandler, Formatter, FileHandler, DEBUG, INFO\n",
    "\n",
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "path_project = Path(os.path.abspath('..'))\n",
    "sys.path.append(str(path_project / 'src'))\n",
    "\n",
    "DATA_SET_DIR = 'dataset'\n",
    "\n",
    "import copy\n",
    "from utils import count_parameters\n",
    "from rdp_accountant import compute_rdp, get_privacy_spent\n",
    "from sampling import mnist_iid, mnist_noniid, mnist_noniid_unequal, client_iid\n",
    "from update import (\n",
    "    LocalUpdate,\n",
    "    l2clipping,\n",
    "    test_inference,\n",
    "    update_global_weights,\n",
    "    client_level_dp_update_global_weights,\n",
    "    diff_weights,\n",
    "    TRAIN_RATIO,\n",
    ")\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from update import DatasetSplit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNIST_CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MNIST_CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 16, 8, 2)\n",
    "        self.conv2 = nn.Conv2d(16, 32, 4, 2)\n",
    "        self.fc1 = nn.Linear(32, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = nn.ZeroPad2d(2)(x)\n",
    "        x = self.conv1(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = self.conv2(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        output = F.log_softmax(x, dim=1)\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "def get_dataset(path_project, num_users, iid=True, all=False):\n",
    "    data_dir = os.path.join(path_project, DATA_SET_DIR, \"mnist\")\n",
    "    apply_transform = transforms.Compose(\n",
    "        [\n",
    "            transforms.ToTensor(),\n",
    "            # https://discuss.pytorch.org/t/normalization-in-the-mnist-example/457\n",
    "            transforms.Normalize((0.1307,), (0.3081,)),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    train_dataset = datasets.MNIST(\n",
    "        data_dir, train=True, download=True, transform=apply_transform\n",
    "    )\n",
    "    test_dataset = datasets.MNIST(\n",
    "        data_dir, train=False, download=True, transform=apply_transform\n",
    "    )\n",
    "\n",
    "    if all:\n",
    "        rand_ids = np.random.permutation(num_users)\n",
    "        user_groups = {\n",
    "            user_id: {record_id} for user_id, record_id in enumerate(rand_ids)\n",
    "        }\n",
    "    elif iid:\n",
    "        user_groups = mnist_iid(train_dataset, num_users)\n",
    "    else:  # args.data_dist == 'non-IID':\n",
    "        user_groups = mnist_noniid(train_dataset, num_users, 5, False)\n",
    "    class_labels = set(test_dataset.train_labels.numpy())\n",
    "\n",
    "    return train_dataset, test_dataset, user_groups, class_labels\n",
    "\n",
    "\n",
    "class CDPLocalUpdate(LocalUpdate):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dataset,\n",
    "        idxs,\n",
    "        logger,\n",
    "        device,\n",
    "        local_bs,\n",
    "        optimizer,\n",
    "        local_lr,\n",
    "        local_ep,\n",
    "        momentum,\n",
    "        verbose,\n",
    "    ):\n",
    "        super().__init__(\n",
    "            dataset,\n",
    "            idxs,\n",
    "            logger,\n",
    "            device,\n",
    "            local_bs,\n",
    "            optimizer,\n",
    "            local_lr,\n",
    "            local_ep,\n",
    "            momentum,\n",
    "            verbose,\n",
    "        )\n",
    "\n",
    "    def train_val_test(self, dataset, idxs):\n",
    "        \"\"\"\n",
    "        Returns train, validation and test dataloaders for a given dataset\n",
    "        and user indexes.\n",
    "        \"\"\"\n",
    "        trainloader = DataLoader(\n",
    "            DatasetSplit(dataset, idxs), batch_size=len(idxs), shuffle=True\n",
    "        )\n",
    "        return trainloader, None\n",
    "\n",
    "\n",
    "class LDPLocalUpdate(LocalUpdate):\n",
    "    def __init__(\n",
    "        self, dataset, idxs, logger, device, optimizer, momentum, verbose, eps, L\n",
    "    ):\n",
    "        self.local_bs = 1\n",
    "        self.optimizer = optimizer\n",
    "        self.lr = None\n",
    "        self.local_ep = 1\n",
    "        self.momentum = momentum\n",
    "        self.verbose = verbose\n",
    "        self.logger = logger\n",
    "        self.eps = eps\n",
    "        self.L = L\n",
    "        self.trainloader = self.train_val_test(dataset, list(idxs))\n",
    "        self.device = device\n",
    "        self.criterion = nn.NLLLoss().to(self.device)\n",
    "\n",
    "    def train_val_test(self, dataset, idxs):\n",
    "        \"\"\"\n",
    "        Returns train, validation and test dataloaders for a given dataset\n",
    "        and user indexes.\n",
    "        \"\"\"\n",
    "        trainloader = DataLoader(\n",
    "            DatasetSplit(dataset, idxs), batch_size=len(idxs), shuffle=True\n",
    "        )\n",
    "        return trainloader\n",
    "\n",
    "    def update_weights(self, model, dp_kind):\n",
    "        model.train()\n",
    "\n",
    "        for batch_idx, (images, labels) in enumerate(self.trainloader):\n",
    "            images, labels = images.to(self.device), labels.to(self.device)\n",
    "            model.zero_grad()\n",
    "            log_probs = model(images)\n",
    "            loss = self.criterion(log_probs, labels)\n",
    "            loss.backward(retain_graph=True)\n",
    "            if dp_kind == \"ldp\":\n",
    "                rand_grad = perturb_grad_ldpsgd(model, self.device, self.eps, self.L)\n",
    "            elif dp_kind == \"nodp\":\n",
    "                rand_grad = grad_sgd(model, self.device)\n",
    "\n",
    "        return model, rand_grad\n",
    "\n",
    "\n",
    "def compute_shuffle_DP(num_users, eps_local, delta, k, delta_global):\n",
    "    # calc shuffling bound by https://arxiv.org/abs/2012.12803\n",
    "    import computeamplification as CA\n",
    "\n",
    "    num_iterations = 10\n",
    "    step = 100\n",
    "    numerical_upperbound_eps = CA.numericalanalysis(\n",
    "        num_users, eps_local, delta, num_iterations, step, True\n",
    "    )\n",
    "\n",
    "    # advanced composition followed by https://arxiv.org/pdf/2001.03618.pdf\n",
    "    e = numerical_upperbound_eps\n",
    "    delta_dash = delta_global - k * delta\n",
    "    if delta_dash < 0:\n",
    "        print(\"##### Delta must be positive\")\n",
    "        return None\n",
    "    cal_eps = k * (e**2) * 0.5 + np.sqrt(k) * e * np.sqrt(\n",
    "        2 * np.log(np.sqrt(k * np.pi * 0.5) * e / delta_dash)\n",
    "    )\n",
    "    return cal_eps\n",
    "\n",
    "\n",
    "def update_server_model_by_local_grad_agg(\n",
    "    base_model,\n",
    "    mean_grad,\n",
    "    dp_kind,\n",
    "    lr_central=1.0,\n",
    "    const_grad=1.0,\n",
    "    param_space_norm=1.0,\n",
    "    scheduler=None,\n",
    "    optimizer=None,\n",
    "):\n",
    "    if dp_kind == \"ldp\":\n",
    "        for name, param in base_model.named_parameters():\n",
    "            if param.requires_grad:\n",
    "                if optimizer is not None:\n",
    "                    param.grad = mean_grad[name] * const_grad\n",
    "                else:\n",
    "                    param.data -= mean_grad[name] * lr_central * const_grad\n",
    "\n",
    "        if optimizer is not None:\n",
    "            optimizer.step()\n",
    "        if scheduler is not None:\n",
    "            scheduler.step()\n",
    "\n",
    "        if param_space_norm > 0:\n",
    "            base_model = l2projection(base_model, param_space_norm)\n",
    "\n",
    "        return base_model\n",
    "\n",
    "    elif dp_kind == \"nodp\":\n",
    "        for name, param in base_model.named_parameters():\n",
    "            if param.requires_grad:\n",
    "                param.grad = mean_grad[name]\n",
    "        optimizer.step()\n",
    "        if scheduler is not None:\n",
    "            scheduler.step()\n",
    "        return base_model\n",
    "\n",
    "\n",
    "def l2projection(model, radius):\n",
    "    l2norm_updated = 0.0\n",
    "    for name, param in model.named_parameters():\n",
    "        if param.requires_grad:\n",
    "            l2norm_updated += param.data.norm(2).item() ** 2\n",
    "    l2norm_updated = l2norm_updated ** (1.0 / 2)\n",
    "\n",
    "    for name, param in model.named_parameters():\n",
    "        if param.requires_grad:\n",
    "            param.data = param.data / max(1.0, l2norm_updated) * radius\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def grad_sgd(model, device):\n",
    "    rand_grad = dict()\n",
    "\n",
    "    for name, param in model.named_parameters():\n",
    "        if param.requires_grad:\n",
    "            rand_grad[name] = param.grad.detach().clone()\n",
    "\n",
    "    model.zero_grad()\n",
    "    return rand_grad\n",
    "\n",
    "\n",
    "# https://anonymous.4open.science/r/ldp-hypothesis-testing-28FA/src/dpsgd_local.py\n",
    "TINYNUM = 1e-10\n",
    "\n",
    "\n",
    "def perturb_grad_ldpsgd(model, device, eps, L):\n",
    "    sample_vec = dict()\n",
    "    rand_grad = dict()\n",
    "\n",
    "    ## To avoid the l2-norm of grad being zero.\n",
    "    for tensor_name, tensor in model.named_parameters():\n",
    "        if tensor.requires_grad:\n",
    "            tensor.grad += (torch.rand(tensor.grad.shape).to(device) - 0.5) * TINYNUM\n",
    "\n",
    "    ## Compute the norm of the clipped gradient, and compy the clipped gradient into the rand_grad\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), L)\n",
    "    l2norm = 0\n",
    "    for tensor_name, tensor in model.named_parameters():\n",
    "        if tensor.requires_grad:\n",
    "            rand_grad[tensor_name] = tensor.grad.detach().clone()\n",
    "            l2norm += rand_grad[tensor_name].norm(2).item() ** 2\n",
    "    model.zero_grad()\n",
    "    l2norm = l2norm ** (1.0 / 2)\n",
    "\n",
    "    ## Compute the sign of first flipping.\n",
    "    p1 = 0.5 + (l2norm / (2 * L))\n",
    "    r1 = np.random.rand(1)\n",
    "    if r1 < p1:\n",
    "        z_sign = 1\n",
    "    else:\n",
    "        z_sign = -1\n",
    "\n",
    "    ## First flipping\n",
    "    for name, param in model.named_parameters():\n",
    "        if param.requires_grad:\n",
    "            rand_grad[name] = rand_grad[name] / l2norm * L * z_sign\n",
    "\n",
    "    ## uniform random vector sampling from L2-ball (unit sphere)\n",
    "    sampvec_sqsum = 0\n",
    "    for name, param in model.named_parameters():\n",
    "        if param.requires_grad:\n",
    "            sample_vec[name] = (\n",
    "                torch.FloatTensor(rand_grad[name].size()).normal_(0, 1).to(device)\n",
    "            )\n",
    "            sampvec_sqsum += torch.sum(sample_vec[name] ** 2)\n",
    "    sampvec_sq = torch.sqrt(sampvec_sqsum)\n",
    "\n",
    "    inner_prod = 0\n",
    "    for name, param in model.named_parameters():\n",
    "        if param.requires_grad:\n",
    "            sample_vec[name] = sample_vec[name] / sampvec_sq\n",
    "            inner_prod += torch.sum(sample_vec[name] * rand_grad[name])\n",
    "\n",
    "    ## Compute the sign of last flipping.\n",
    "    last_sign = torch.sign(inner_prod)\n",
    "    p2 = np.exp(eps) / (np.exp(eps) + 1)\n",
    "    r2 = np.random.rand(1)\n",
    "    if r2 > p2:\n",
    "        last_sign *= -1\n",
    "\n",
    "    ## Last flipping for ensuring eps-LDP\n",
    "    for name, param in model.named_parameters():\n",
    "        if param.requires_grad:\n",
    "            sample_vec[name] = sample_vec[name] * last_sign\n",
    "\n",
    "    return sample_vec\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FedAVG\n",
    "- client-level DP FedAVG\n",
    "- no dp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_fed_avg(\n",
    "    seed,\n",
    "    gpu_id,\n",
    "    logger,\n",
    "    num_users,\n",
    "    frac,\n",
    "    sigma,\n",
    "    clipping,\n",
    "    epochs,\n",
    "    epsilon,\n",
    "    delta,\n",
    "    local_bs,\n",
    "    optimizer,\n",
    "    local_lr,\n",
    "    local_ep,\n",
    "    momentum,\n",
    "    verbose,\n",
    "    dp_kind,\n",
    "):\n",
    "    if gpu_id:\n",
    "        torch.cuda.set_device(gpu_id)\n",
    "    device = \"cuda\" if gpu_id else \"cpu\"\n",
    "\n",
    "    print(\"DP: \", dp_kind)\n",
    "\n",
    "    global_model = MNIST_CNN()\n",
    "    global_model.to(device)\n",
    "    global_model.train()\n",
    "\n",
    "    global_weights = global_model.state_dict()\n",
    "    train_dataset, test_dataset, user_groups, class_labels = get_dataset(\n",
    "        path_project, num_users, True, num_users == 60000\n",
    "    )\n",
    "    rs_for_gaussian_noise = np.random.RandomState(seed)\n",
    "\n",
    "    # Training\n",
    "    global_test_result = []\n",
    "\n",
    "    # CDP\n",
    "    orders = (\n",
    "        [1.25, 1.5, 1.75, 2.0, 2.25, 2.5, 3.0, 3.5, 4.0, 4.5]\n",
    "        + list(range(5, 64))\n",
    "        + [128, 256, 512]\n",
    "    )\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        if dp_kind == \"cdp\":\n",
    "            rdp = compute_rdp(frac, sigma, epoch + 1, orders)\n",
    "            eps_spent, delta_spent, opt_order = get_privacy_spent(\n",
    "                orders, rdp, target_delta=delta\n",
    "            )\n",
    "            if eps_spent > epsilon or delta_spent > delta:\n",
    "                print(\"######## Excess setted privacy budget ########\")\n",
    "                # break\n",
    "\n",
    "        local_weights_diffs, local_losses = [], []\n",
    "        global_model.train()\n",
    "\n",
    "        idxs_users = client_iid(frac, num_users)\n",
    "\n",
    "        for idx in idxs_users:\n",
    "            local_model = CDPLocalUpdate(\n",
    "                dataset=train_dataset,\n",
    "                idxs=user_groups[idx],\n",
    "                logger=logger,\n",
    "                device=device,\n",
    "                local_bs=local_bs,\n",
    "                optimizer=optimizer,\n",
    "                local_lr=local_lr,\n",
    "                local_ep=local_ep,\n",
    "                momentum=momentum,\n",
    "                verbose=verbose,\n",
    "            )\n",
    "\n",
    "            w, loss = local_model.update_weights(\n",
    "                model=copy.deepcopy(global_model), global_round=epoch\n",
    "            )\n",
    "            local_weights_diffs.append(diff_weights(global_weights, w))\n",
    "            local_losses.append(copy.deepcopy(loss))\n",
    "\n",
    "        if dp_kind == \"cdp\":\n",
    "            client_level_dp_update_global_weights(\n",
    "                global_weights,\n",
    "                local_weights_diffs,\n",
    "                sigma,\n",
    "                clipping,\n",
    "                None,\n",
    "                rs_for_gaussian_noise,\n",
    "            )\n",
    "        elif dp_kind == \"nodp\":\n",
    "            update_global_weights(global_weights, local_weights_diffs)\n",
    "\n",
    "        global_model.load_state_dict(global_weights)\n",
    "\n",
    "        # Calculate avg training accuracy over all users at every epoch\n",
    "        global_model.eval()\n",
    "        test_acc, test_loss = test_inference(global_model, test_dataset, device)\n",
    "        print(f\" \\n Results after {epoch} ({epochs}) global rounds of training:\")\n",
    "        print(\"|---- Test Accuracy: {:.2f}%\".format(100 * test_acc))\n",
    "        global_test_result.append((test_acc, test_loss))\n",
    "        if dp_kind == \"cdp\":\n",
    "            print(\n",
    "                \"|---- Central DP : ({:.6f}, {:.6f})-DP\".format(eps_spent, delta_spent)\n",
    "            )\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FedSVG\n",
    "- with ldp-sgd (https://arxiv.org/pdf/2001.03618.pdf)\n",
    "- no dp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_fed_sgd(\n",
    "    seed,\n",
    "    gpu_id,\n",
    "    logger,\n",
    "    num_users,\n",
    "    frac,\n",
    "    epochs,\n",
    "    delta,\n",
    "    optimizer,\n",
    "    momentum,\n",
    "    eps_local,\n",
    "    verbose,\n",
    "    dp_kind,\n",
    "):\n",
    "    if gpu_id:\n",
    "        torch.cuda.set_device(gpu_id)\n",
    "    device = \"cuda\" if gpu_id else \"cpu\"\n",
    "    print(\"DP: \", dp_kind)\n",
    "    if dp_kind == \"ldp\":\n",
    "        print(f\"    {eps_local}-LDP for local randomizer\")\n",
    "\n",
    "    global_model = MNIST_CNN()\n",
    "    global_model.to(device)\n",
    "    global_model.train()\n",
    "\n",
    "    train_dataset, test_dataset, user_groups, class_labels = get_dataset(\n",
    "        path_project, num_users, True, num_users == 60000\n",
    "    )\n",
    "\n",
    "    # Training\n",
    "    global_test_result = []\n",
    "\n",
    "    if dp_kind == \"ldp\":\n",
    "        d = 0\n",
    "        L = 1.0\n",
    "        for name, param in global_model.named_parameters():\n",
    "            if param.requires_grad:\n",
    "                d += param.data.numel()\n",
    "        const_gamma = np.exp(loggamma((d - 1) * 0.5 + 1) - loggamma(d * 0.5 + 1))\n",
    "        const_eps = (np.exp(eps_local) + 1) / (np.exp(eps_local) - 1)\n",
    "        const_grad = L * np.sqrt(np.pi) * 0.5 * const_gamma * const_eps * d\n",
    "        param_space_norm = L * const_eps * 0.75 * np.sqrt(np.pi) * np.sqrt(d)\n",
    "        lr_central = (\n",
    "            param_space_norm * np.sqrt(num_users) / (const_eps * L * np.sqrt(d)) * 0.05\n",
    "        )\n",
    "\n",
    "    elif dp_kind == \"nodp\":\n",
    "        lr_central = 0.1\n",
    "        L = None\n",
    "        param_space_norm = None\n",
    "        const_grad = None\n",
    "\n",
    "    if dp_kind == \"ldp\":\n",
    "        if param_space_norm > 0:\n",
    "            global_model = l2projection(global_model, param_space_norm)\n",
    "\n",
    "    opt = optim.SGD(global_model.parameters(), lr=lr_central, momentum=0.0)\n",
    "    scheduler = optim.lr_scheduler.StepLR(opt, step_size=100, gamma=0.5)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        global_model.train()\n",
    "\n",
    "        idxs_users = client_iid(frac, num_users)\n",
    "        mean_grad = dict()\n",
    "\n",
    "        for idx in idxs_users:\n",
    "            local_model = LDPLocalUpdate(\n",
    "                dataset=train_dataset,\n",
    "                idxs=user_groups[idx],\n",
    "                logger=logger,\n",
    "                device=device,\n",
    "                optimizer=optimizer,\n",
    "                momentum=momentum,\n",
    "                verbose=verbose,\n",
    "                eps=eps_local,\n",
    "                L=L,\n",
    "            )\n",
    "            model, rand_grad = local_model.update_weights(\n",
    "                model=copy.deepcopy(global_model), dp_kind=dp_kind\n",
    "            )\n",
    "\n",
    "            for name in rand_grad.keys():\n",
    "                if mean_grad.get(name) is None:\n",
    "                    mean_grad[name] = rand_grad[name]\n",
    "                else:\n",
    "                    mean_grad[name] += rand_grad[name]\n",
    "\n",
    "        for name in mean_grad.keys():\n",
    "            mean_grad[name] /= len(idxs_users)\n",
    "\n",
    "        # print(mean_grad[name] * const_grad * lr_central)\n",
    "\n",
    "        global_model = update_server_model_by_local_grad_agg(\n",
    "            global_model,\n",
    "            mean_grad,\n",
    "            dp_kind,\n",
    "            lr_central=lr_central,\n",
    "            optimizer=opt,\n",
    "            const_grad=const_grad,\n",
    "            scheduler=scheduler,\n",
    "            param_space_norm=param_space_norm,\n",
    "        )\n",
    "\n",
    "        test_acc, test_loss = test_inference(global_model, test_dataset, device)\n",
    "        print(f\" \\n Results after {epoch} ({epochs}) global rounds of training:\")\n",
    "        print(\"|---- Test Accuracy: {:.2f}%\".format(100 * test_acc))\n",
    "        global_test_result.append((epoch, test_acc, test_loss))\n",
    "        if dp_kind == \"ldp\":\n",
    "            individual_delta = (delta / 2.0) / (epoch + 1)\n",
    "            shuffle_dp_eps = compute_shuffle_DP(\n",
    "                num_users, eps_local, individual_delta, epoch + 1, delta\n",
    "            )\n",
    "            print(\n",
    "                \"|---- Shuffle DP : ({:.6f}, {:.6f})-DP\".format(shuffle_dp_eps, delta)\n",
    "            )\n",
    "\n",
    "    f = open(f\"results/fedsgd-{dp_kind}-{eps_local}.txt\", \"w\")\n",
    "    f.write(str(global_test_result))\n",
    "    f.close()\n",
    "    print(\"done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## parameter\n",
    "seed=0\n",
    "gpu_id=None\n",
    "num_users=60000\n",
    "frac=1.0\n",
    "epochs=300\n",
    "delta=0.00001\n",
    "clipping=1.0\n",
    "eps_central=5.0\n",
    "optimizer=\"sgd\"\n",
    "eps_local=1.9\n",
    "sigma=1.12\n",
    "local_bs=10\n",
    "local_lr=0.01\n",
    "local_ep=10\n",
    "momentum=0.5\n",
    "dp_kind=\"ldp\"\n",
    "method=\"fedsgd\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "logger = SummaryWriter(os.path.join(path_project, \"log\"))\n",
    "\n",
    "if method == \"fedsgd\":\n",
    "    if dp_kind in [\"ldp\", \"nodp\"]:\n",
    "        eval_fed_sgd(\n",
    "            seed=seed,\n",
    "            gpu_id=gpu_id,\n",
    "            logger=logger,\n",
    "            num_users=num_users,\n",
    "            frac=frac,\n",
    "            epochs=epochs,\n",
    "            delta=delta,\n",
    "            optimizer=\"sgd\",\n",
    "            momentum=0.0,\n",
    "            eps_local=eps_local,\n",
    "            verbose=False,\n",
    "            dp_kind=dp_kind,\n",
    "        )\n",
    "    else:\n",
    "        exit(\"Error: dp_kind must be ldp, nodp for fedsvg\")\n",
    "\n",
    "elif method == \"fedavg\":\n",
    "    if dp_kind in [\"cdp\", \"nodp\"]:\n",
    "        eval_fed_avg(\n",
    "            seed=seed,\n",
    "            gpu_id=gpu_id,\n",
    "            logger=logger,\n",
    "            num_users=num_users,\n",
    "            frac=frac,\n",
    "            epochs=epochs,\n",
    "            sigma=sigma,\n",
    "            clipping=clipping,\n",
    "            epsilon=eps_central,\n",
    "            delta=delta,\n",
    "            local_bs=1,\n",
    "            local_ep=5,\n",
    "            local_lr=local_lr,\n",
    "            optimizer=\"sgd\",\n",
    "            momentum=momentum,\n",
    "            verbose=False,\n",
    "            dp_kind=dp_kind,\n",
    "        )\n",
    "    else:\n",
    "        exit(\"Error: dp_kind must be cdp, nodp for fedavg\")\n",
    "else:\n",
    "    exit(\"Error: no method\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fl-tee",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
